<!DOCTYPE html>
<html>
    <head>
        <script src="//treetopflyer.github.com/vcore/lib.js"></script>
<script>

var Neuron = {};
Neuron.Create = function(inSize)
{
    var i;
    var obj = {};
    
    obj.Weights = [];
    for(i=0; i<inSize; i++)
    {
        obj.Weights[i] = Math.random();
    }
    obj.Bias = Math.random();
    
    
    obj.Linear = function(inInput)
    {
        var i;
        var sum = 0;
        
        for(i=0; i<obj.Weights.length; i++)
            sum += obj.Weights[i]*inInput[i];
            
        return sum + obj.Bias;
    };
    obj.Sigmoid = function(inLinear)
    {
        return 1 / (1 + Math.pow(Math.E, -inLinear));
    };
    obj.Derivative = function(inSigmoid)
    {
          return inSigmoid * (1 - inSigmoid);
    };
    
    obj.Learn = function(inInput, inOutput)
    {
        var i;
        var linear = obj.Linear(inInput);
        var sigmoid = obj.Sigmoid(linear);
        var derivative = obj.Derivative(sigmoid);
        var miss = sigmoid - inOutput;
        
        for(i=0; i<obj.Weights.length; i++)
        {
            obj.Weights[i] -= 0.1 * miss * derivative * inInput[i];
        }
        obj.Bias -= 0.1 * miss * derivative * 1;
    };
    obj.Observe = function(inInput)
    {
        return obj.Sigmoid(obj.Linear(inInput));
    };
    
    return obj;
}

var n1 = Neuron.Create(2);
var i;
for(i=0; i<1000; i++)
{
    n1.Learn([0, 0], 0);
    n1.Learn([1, 1], 1);
}

</script>
    </head>
    <body>
        <script>
            
var NN = {};

NN.TrainingSet = {};
NN.TrainingSet.Instances = [];
NN.TrainingSet.Create = function()
{
    var obj = {};

    obj.Input = [];
    obj.Output = [];
    obj.Order = [];
    
    NN.TrainingSet.Instances.push(obj);
    return obj;
};

NN.TrainingSet.Add = function(inTrainingSet, inType, inData)
{
    inTrainingSet.Input.push(inData);
    inTrainingSet.Output.push(inType);
    inTrainingSet.Order.push(inTrainingSet.Order.length);
};
NN.TrainingSet.Randomize = function(inTrainingSet)
{
      var newOrder = [];
      var selection;
      while(inTrainingSet.Order.length != 0)
      {
          selection = Math.floor(inTrainingSet.Order.length * Math.random());
          inTrainingSet.Order.splice(selection, 1);
          newOrder.push(selection);
      }
      inTrainingSet.Order = newOrder;
};




NN.Network = {};
NN.Network.Instances = [];
NN.Network.Create = function()
{
    var obj = {};
    var i;    
    
    obj.Layers = [];
    obj.LearningRate = 0.1;
    obj.Error = [];

    for(i=0; i<arguments.length-1; i++)
    {
        obj.Layers.push(NN.Layer.Create(arguments[i], arguments[i+1]));
    }
    
    NN.Network.Instances.push(obj);
    return obj;
};
NN.Network.Observe = function(inNetwork, inBatch)
{
      var input = inBatch;
      var i;
      for(i=0; i<inNetwork.Layers.length; i++)
      {
          input = NN.Layer.Forward(inNetwork.Layers[i], input);
      }
      return inNetwork.Layers[inNetwork.Layers.length-1].Forward.StageSigmoid;
};
NN.Network.Error = function(inNetwork, inTraining)
{
      return M.Subtract(inNetwork.Layers[inNetwork.Layers.length-1].Forward.StageSigmoid,  inTraining);
};
NN.Network.Learn = function(inNetwork, inError)
{
      var input = inError;
      var i;
      for(i=inNetwork.Layers.length-1; i>=0; i--)
      {
          input = NN.Layer.Backward(inNetwork.Layers[i], input);
          NN.Layer.Adjust(inNetwork.Layers[i], inNetwork.LearningRate);
      }
};


NN.Network.Batch = function(inNetwork, inTrainingSet, inIterations)
{
    var i;
    for(i=0; i<inIterations; i++)
    {
        NN.Network.Observe(inNetwork, inTrainingSet.Input);
        inNetwork.Error = NN.Network.Error(inNetwork, inTrainingSet.Output)
        NN.Network.Learn(inNetwork, inNetwork.Error);
    }
};
NN.Network.Stochastic = function(inNetwork, inTrainingSet, inIterations)
{
    var i, j;
    var current;
    for(i=0; i<inIterations; i++)
    {
        NN.TrainingSet.Randomize(inTrainingSet);
        for(j=0; j<inTrainingSet.Order.length; j++)
        {
            current = inTrainingSet.Order[j];
            NN.Network.Observe(inNetwork, [inTrainingSet.Input[current]]);
            inNetwork.Error = NN.Network.Error(inNetwork, [inTrainingSet.Output[current]]);
            NN.Network.Learn(inNetwork, inNetwork.Error);
        }
    }
};



NN.Layer = {};
NN.Layer.Create = function(sizeIn, sizeOut)
{
    var i;
    var min = [];
    var max = [];
    var obj = {};
    
    sizeIn++;
    
    obj.Forward = {};
    for(i=0; i<sizeIn; i++)
    {
        min.push(0);
        max.push(1);
    }
    obj.Forward.Matrix = M.Box([min, max], sizeOut);
    obj.Forward.StageInput = [];
    obj.Forward.StageAffine = [];
    obj.Forward.StageSigmoid = [];
    obj.Forward.StageDerivative = [];
    
    obj.Backward = {};
    obj.Backward.Matrix = M.Transpose(obj.Forward.Matrix);
    obj.Backward.StageInput = [];
    obj.Backward.StageDerivative = [];
    obj.Backward.StageAffine = [];
    
    return obj;
};
NN.Layer.Forward = function(inLayer, inInput)
{
    inLayer.Forward.StageInput = M.Pad(inInput); // Pad the input
    inLayer.Forward.StageAffine = M.Transform(inLayer.Forward.Matrix, inLayer.Forward.StageInput);
    inLayer.Forward.StageSigmoid = M.Sigmoid(inLayer.Forward.StageAffine);
    inLayer.Forward.StageDerivative = M.Derivative(inLayer.Forward.StageSigmoid);
    
    return inLayer.Forward.StageSigmoid;
};
NN.Layer.Error = function(inLayer, inTarget)
{
    return M.Subtract(inLayer.Forward.StageSigmoid, inTarget);
};
NN.Layer.Backward = function(inLayer, inInput)
{
    inLayer.Backward.StageInput = inInput;
    inLayer.Backward.StageDerivative = M.Multiply(inLayer.Backward.StageInput, inLayer.Forward.StageDerivative)
    inLayer.Backward.Matrix = M.Transpose(inLayer.Forward.Matrix);
    inLayer.Backward.StageAffine = M.Transform(inLayer.Backward.Matrix, inLayer.Backward.StageDerivative);
    
    return M.Unpad(inLayer.Backward.StageAffine);// Unpad the output
};
NN.Layer.Adjust = function(inLayer, inLearningRate)
{
    var deltas;
    var vector;
    var scalar;
    var batchSize = inLayer.Forward.StageInput.length;
    var i, j;
    
    for(i=0; i<batchSize; i++)
    {
        deltas = [];
        
        vector = inLayer.Forward.StageInput[i];
        for(j=0; j<inLayer.Forward.Matrix.length; j++)
        {
            deltas[j] = V.Scale(vector, (inLayer.Backward.StageDerivative[i][j] * inLearningRate) / batchSize);
        }
        
        inLayer.Forward.Matrix = M.Subtract(inLayer.Forward.Matrix, deltas);
    }
};
NN.Layer.Stochastic = function(inLayer, inTrainingSet, inIterations)
{
    var i, j;
    var current;
    var error;
    for(i=0; i<inIterations; i++)
    {
        NN.TrainingSet.Randomize(inTrainingSet);
        for(j=0; j<inTrainingSet.Order.length; j++)
        {
            current = inTrainingSet.Order[j];
            NN.Layer.Forward(inLayer, [inTrainingSet.Input[current]]);
            error = M.Subtract(inLayer.Forward.StageSigmoid, [inTrainingSet.Output[current]]);
            NN.Layer.Backward(inLayer, error);
            NN.Layer.Adjust(inLayer, 0.1);
        }
    }
};


        </script>
        <script>

var ts1 = NN.TrainingSet.Create();
NN.TrainingSet.Add(ts1, [0], [0, 0]);
NN.TrainingSet.Add(ts1, [0], [10, 5]);
NN.TrainingSet.Add(ts1, [0], [-8, 10]);
NN.TrainingSet.Add(ts1, [0], [-20, 5]);

NN.TrainingSet.Add(ts1, [1], [120, 111]);
NN.TrainingSet.Add(ts1, [1], [80, 130]);
NN.TrainingSet.Add(ts1, [1], [75, 190]);
NN.TrainingSet.Add(ts1, [1], [130, 80]);


var error;
var l1 = NN.Layer.Create(2, 1);
l1.Forward.Matrix = [];
l1.Forward.Matrix.push([0.2, 0.1, 0.8]);

function test()
{
    NN.Layer.Forward(l1, [[0, 0], [1, 1]]);
    error = NN.Layer.Error(l1, [[0], [1]]);
    NN.Layer.Backward(l1, error);
    NN.Layer.Adjust(l1, 0.1);
}

function bigTest()
{
    var i;
    for(i=0; i<1000; i++)
    {
        test();
    }
}

bigTest();


/*
console.log(NN.Layer.Forward(l1, [[0, 0]]));
console.log(NN.Layer.Forward(l1, [[1, 1]]));
NN.Layer.Stochastic(l1, ts1, 500);
console.log(NN.Layer.Forward(l1, [[0, 0]]));
console.log(NN.Layer.Forward(l1, [[1, 1]]));
*/



        </script>
    </body>
</html>